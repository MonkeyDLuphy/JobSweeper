{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explict recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pymongo_spark:\n",
      "\n",
      "NAME\n",
      "    pymongo_spark\n",
      "\n",
      "FILE\n",
      "    /home/cloudera/mongo-hadoop/spark/src/main/python/pymongo_spark.py\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright 2015 MongoDB, Inc.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    # http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "FUNCTIONS\n",
      "    BSONFilePairRDD(self, file_path, config=None)\n",
      "        Create a pair RDD backed by a BSON file.\n",
      "    \n",
      "    BSONFileRDD(self, file_path, config=None)\n",
      "        Create an RDD backed by a BSON file.\n",
      "    \n",
      "    activate()\n",
      "        Activate integration between PyMongo and PySpark.\n",
      "        This function only needs to be called once.\n",
      "    \n",
      "    mongoPairRDD(self, connection_string, config=None)\n",
      "        Create a pair RDD backed by MongoDB.\n",
      "    \n",
      "    mongoRDD(self, connection_string, config=None)\n",
      "        Create an RDD backed by MongoDB.\n",
      "    \n",
      "    saveToBSON(self, file_path, config=None)\n",
      "        Save this RDD as a BSON file.\n",
      "    \n",
      "    saveToMongoDB(self, connection_string, config=None)\n",
      "        Save this RDD to MongoDB.\n",
      "\n",
      "DATA\n",
      "    __version__ = '0.1'\n",
      "\n",
      "VERSION\n",
      "    0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymongo_spark\n",
    "pymongo_spark.activate()\n",
    "help(pymongo_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.638447642646\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Load and parse the data\n",
    "#sc = pyspark.SparkContext( appName=\"recommend\" ,master=\"local[*]\")\n",
    "data = sc.textFile(\"File:/home/cloudera/Desktop/u.data\")    #read file at localsite\n",
    "ratings = data.map(lambda l: l.split('\\t'))\\\n",
    "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank =5\n",
    "numIterations = 5\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"/home/cloudera/Desktop/myCollaborativeFilter\")\n",
    "sameModel = MatrixFactorizationModel.load(sc, \"/home/cloudera/Desktop/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"File:/home/cloudera/Desktop/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(727, 68) 3.15003492761\n"
     ]
    }
   ],
   "source": [
    "print (predictions.collectAsMap()).keys()[0], (predictions.collectAsMap()).values()[0]\n",
    "#predictions.saveToMongoDB(\"mongodb://10.120.28.17:27017/test.webs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic_list = []\n",
    "dic_key = predictions.collectAsMap().keys()\n",
    "dic_val = predictions.collectAsMap().values()\n",
    "for i in xrange(0,100000):\n",
    "    userid= dic_key[i][0]\n",
    "    movid = dic_key[i][1]\n",
    "    rating = dic_val[i]\n",
    "    #print userid,movid,rating\n",
    "    dic = {\"userid\":userid,\"movid\":movid,\"rating\":rating}\n",
    "    dic_list.append(dic)\n",
    "\n",
    "\n",
    "# model {\"userid\":727,\"movid\":68,\"rating\":3.15003492761}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/lib/python2.7/site-packages/ipykernel/__main__.py:7: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('10.120.28.124', 27017)   #輸入mongodb主機IP與port,連自己主機可以空白,\n",
    "database = client[\"test\"]                     #[]填入自己設定的db name\n",
    "collection = database[\"rdd\"]                 #[]填入自己設定的collection name\n",
    "collection.insert(                       #新增資料進去\n",
    "        dic_list\n",
    "    )\n",
    "client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('10.120.28.124', 27017)   #輸入mongodb主機IP與port,連自己主機可以空白,\n",
    "database = client[\"test\"]                     #[]填入自己設定的db name\n",
    "collection = database[\"webs\"]\n",
    "\n",
    "collection.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implict  recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.480799177669\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1067.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory File:/home/cloudera/Desktop/u1/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1426)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1405)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1405)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1405)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:360)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:205)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-aa1e965a9c55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Save and load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"File:/home/cloudera/Desktop/u1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0msameModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"File:/home/cloudera/Desktop/u1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/mllib/util.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sc, path)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1067.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory File:/home/cloudera/Desktop/u1/metadata already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1426)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1405)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1405)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1405)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel$SaveLoadV1_0$.save(MatrixFactorizationModel.scala:360)\n\tat org.apache.spark.mllib.recommendation.MatrixFactorizationModel.save(MatrixFactorizationModel.scala:205)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Load and parse the data\n",
    "#sc = pyspark.SparkContext( appName=\"recommend\" ,master=\"local[*]\")\n",
    "data = sc.textFile(\"File:/home/cloudera/Desktop/u1.data\")\n",
    "ratings = data.map(lambda l: l.split('\\t'))\\\n",
    "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank =5\n",
    "numIterations = 5\n",
    "model1 = ALS.trainImplicit(ratings, rank, numIterations,0.1)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions1 = model1.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions1)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model1.save(sc, \"File:/home/cloudera/Desktop/u1\")\n",
    "sameModel = MatrixFactorizationModel.load(sc, \"File:/home/cloudera/Desktop/u1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MatrixFactorizationModel in module pyspark.mllib.recommendation object:\n",
      "\n",
      "class MatrixFactorizationModel(pyspark.mllib.common.JavaModelWrapper, pyspark.mllib.util.JavaSaveable, pyspark.mllib.util.JavaLoader)\n",
      " |  A matrix factorisation model trained by regularized alternating\n",
      " |  least-squares.\n",
      " |  \n",
      " |  >>> r1 = (1, 1, 1.0)\n",
      " |  >>> r2 = (1, 2, 2.0)\n",
      " |  >>> r3 = (2, 1, 2.0)\n",
      " |  >>> ratings = sc.parallelize([r1, r2, r3])\n",
      " |  >>> model = ALS.trainImplicit(ratings, 1, seed=10)\n",
      " |  >>> model.predict(2, 2)\n",
      " |  0.4...\n",
      " |  \n",
      " |  >>> testset = sc.parallelize([(1, 2), (1, 1)])\n",
      " |  >>> model = ALS.train(ratings, 2, seed=0)\n",
      " |  >>> model.predictAll(testset).collect()\n",
      " |  [Rating(user=1, product=1, rating=1.0...), Rating(user=1, product=2, rating=1.9...)]\n",
      " |  \n",
      " |  >>> model = ALS.train(ratings, 4, seed=10)\n",
      " |  >>> model.userFeatures().collect()\n",
      " |  [(1, array('d', [...])), (2, array('d', [...]))]\n",
      " |  \n",
      " |  >>> model.recommendUsers(1, 2)\n",
      " |  [Rating(user=2, product=1, rating=1.9...), Rating(user=1, product=1, rating=1.0...)]\n",
      " |  >>> model.recommendProducts(1, 2)\n",
      " |  [Rating(user=1, product=2, rating=1.9...), Rating(user=1, product=1, rating=1.0...)]\n",
      " |  >>> model.rank\n",
      " |  4\n",
      " |  \n",
      " |  >>> first_user = model.userFeatures().take(1)[0]\n",
      " |  >>> latents = first_user[1]\n",
      " |  >>> len(latents) == 4\n",
      " |  True\n",
      " |  \n",
      " |  >>> model.productFeatures().collect()\n",
      " |  [(1, array('d', [...])), (2, array('d', [...]))]\n",
      " |  \n",
      " |  >>> first_product = model.productFeatures().take(1)[0]\n",
      " |  >>> latents = first_product[1]\n",
      " |  >>> len(latents) == 4\n",
      " |  True\n",
      " |  \n",
      " |  >>> model = ALS.train(ratings, 1, nonnegative=True, seed=10)\n",
      " |  >>> model.predict(2, 2)\n",
      " |  3.8...\n",
      " |  \n",
      " |  >>> df = sqlContext.createDataFrame([Rating(1, 1, 1.0), Rating(1, 2, 2.0), Rating(2, 1, 2.0)])\n",
      " |  >>> model = ALS.train(df, 1, nonnegative=True, seed=10)\n",
      " |  >>> model.predict(2, 2)\n",
      " |  3.8...\n",
      " |  \n",
      " |  >>> model = ALS.trainImplicit(ratings, 1, nonnegative=True, seed=10)\n",
      " |  >>> model.predict(2, 2)\n",
      " |  0.4...\n",
      " |  \n",
      " |  >>> import os, tempfile\n",
      " |  >>> path = tempfile.mkdtemp()\n",
      " |  >>> model.save(sc, path)\n",
      " |  >>> sameModel = MatrixFactorizationModel.load(sc, path)\n",
      " |  >>> sameModel.predict(2, 2)\n",
      " |  0.4...\n",
      " |  >>> sameModel.predictAll(testset).collect()\n",
      " |  [Rating(...\n",
      " |  >>> from shutil import rmtree\n",
      " |  >>> try:\n",
      " |  ...     rmtree(path)\n",
      " |  ... except OSError:\n",
      " |  ...     pass\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MatrixFactorizationModel\n",
      " |      pyspark.mllib.common.JavaModelWrapper\n",
      " |      pyspark.mllib.util.JavaSaveable\n",
      " |      pyspark.mllib.util.Saveable\n",
      " |      pyspark.mllib.util.JavaLoader\n",
      " |      pyspark.mllib.util.Loader\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  predict(self, user, product)\n",
      " |      Predicts rating for the given user and product.\n",
      " |  \n",
      " |  predictAll(self, user_product)\n",
      " |      Returns a list of predicted ratings for input user and product pairs.\n",
      " |  \n",
      " |  productFeatures(self)\n",
      " |      Returns a paired RDD, where the first element is the product and the\n",
      " |      second is an array of features corresponding to that product.\n",
      " |  \n",
      " |  recommendProducts(self, user, num)\n",
      " |      Recommends the top \"num\" number of products for a given user and returns a list\n",
      " |      of Rating objects sorted by the predicted rating in descending order.\n",
      " |  \n",
      " |  recommendUsers(self, product, num)\n",
      " |      Recommends the top \"num\" number of users for a given product and returns a list\n",
      " |      of Rating objects sorted by the predicted rating in descending order.\n",
      " |  \n",
      " |  userFeatures(self)\n",
      " |      Returns a paired RDD, where the first element is the user and the\n",
      " |      second is an array of features corresponding to that user.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(cls, sc, path) from __builtin__.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  rank\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.mllib.common.JavaModelWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, java_model)\n",
      " |  \n",
      " |  call(self, name, *a)\n",
      " |      Call method of java_model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.mllib.common.JavaModelWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.mllib.util.JavaSaveable:\n",
      " |  \n",
      " |  save(self, sc, path)\n",
      " |      Save this model to the given path.\n",
      " |      \n",
      " |      This saves:\n",
      " |       * human-readable (JSON) model metadata to path/metadata/\n",
      " |       * Parquet formatted data to path/data/\n",
      " |      \n",
      " |      The model may be loaded using py:meth:`Loader.load`.\n",
      " |      \n",
      " |      :param sc: Spark context used to save model data.\n",
      " |      :param path: Path specifying the directory in which to save\n",
      " |                   this model. If the directory already exists,\n",
      " |                   this method throws an exception.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "print model1.recommendProducts(1,5)[0].product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363, 849) 0.29318396615\n"
     ]
    }
   ],
   "source": [
    "print (predictions1.collectAsMap()).keys()[2], (predictions.collectAsMap()).values()[2]\n",
    "#predictions.saveToMongoDB(\"mongodb://10.120.28.17:27017/test.webs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic_list = []\n",
    "dic_key = predictions1.collectAsMap().keys()\n",
    "dic_val = predictions1.collectAsMap().values()\n",
    "with open('test.txt','a+') as ff:\n",
    "    for i in xrange(0,100000):\n",
    "        userid= dic_key[i][0]\n",
    "        movid = dic_key[i][1]\n",
    "        rating = dic_val[i]\n",
    "        #print userid,movid,rating\n",
    "        dic = {\"userid\":userid,\"movid\":movid,\"rating\":rating}\n",
    "        dic_list.append(dic)\n",
    "        b = str(userid)+\",\"+str(movid)+\",\"+str(rating)+\"\\n\"\n",
    "        ff.write(b)\n",
    "    ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/lib/python2.7/site-packages/ipykernel/__main__.py:7: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "10.120.28.17:27017: timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-564a224456e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcollection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"rdd\"\u001b[0m\u001b[1;33m]\u001b[0m                 \u001b[1;31m#[]填入自己設定的collection name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m collection.insert(                       #新增資料進去\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdic_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python/lib/python2.7/site-packages/pymongo-3.2.2-py2.7-linux-x86_64.egg/pymongo/collection.pyc\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, doc_or_docs, manipulate, check_keys, continue_on_error, **kwargs)\u001b[0m\n\u001b[0;32m   2201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m             \u001b[0mwrite_concern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWriteConcern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2203\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_writes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2204\u001b[0m             return self._insert(sock_info, doc_or_docs, not continue_on_error,\n\u001b[0;32m   2205\u001b[0m                                 check_keys, manipulate, write_concern)\n",
      "\u001b[1;32m/usr/local/python/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python/lib/python2.7/site-packages/pymongo-3.2.2-py2.7-linux-x86_64.egg/pymongo/mongo_client.pyc\u001b[0m in \u001b[0;36m_get_socket\u001b[1;34m(self, selector)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m         \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python/lib/python2.7/site-packages/pymongo-3.2.2-py2.7-linux-x86_64.egg/pymongo/topology.pyc\u001b[0m in \u001b[0;36mselect_server\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    140\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[0;32m    141\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                                                  address))\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[1;32m/usr/local/python/lib/python2.7/site-packages/pymongo-3.2.2-py2.7-linux-x86_64.egg/pymongo/topology.pyc\u001b[0m in \u001b[0;36mselect_servers\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[1;32m--> 118\u001b[1;33m                         self._error_message(selector))\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m: 10.120.28.17:27017: timed out"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('10.120.28.17', 27017)   #輸入mongodb主機IP與port,連自己主機可以空白,\n",
    "database = client[\"test\"]                     #[]填入自己設定的db name\n",
    "collection = database[\"rdd\"]                 #[]填入自己設定的collection name\n",
    "collection.insert(                       #新增資料進去\n",
    "        dic_list\n",
    "    )\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
